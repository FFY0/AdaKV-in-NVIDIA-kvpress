{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head-specific compression\n",
    "In this notebook, we demonstrate how to implement head-specific compression using an example of Ada-SnapKV from (AdaKV)[arxiv.org/abs/2407.11550]. Before proceeding, please refer to the file `new_press.ipynb` to understand how standard compression works.\n",
    "\n",
    "### Key observation\n",
    "\n",
    "Different attention heads within LLMs exhibit significant disparities in their attention patterns, such as varying degrees of attention concentration. This enables us to distribute the overall budget across different attention heads strategically. For example, according to AdaKV, we allocate more budget to attention heads with distributed attention and reduce the compression budget for heads exhibiting concentrated attention.\n",
    "\n",
    "### How to Achieve Head-specific Compression in Practice?\n",
    "Head-specific compression offers significant advantages under the same budget compared to standard compression. However, it introduces challenges in managing cache length differences across heads. To address these challenges, we provide a solution based on a flattened cache layout `DynamicCacheSplitHeadFlatten`, complemented by:\n",
    "\n",
    "* Custom CUDA kernels for cache update operations: `update_flatten_klenN_view`\n",
    "* Flash Attention techniques supporting variable-length cache computations:  `flash_attn_varlen_func`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import pipeline\n",
    "from kvpress import BasePress, AdaSnapKVPress\n",
    "\n",
    "context = \"In this step-by-step guide, you will learn how to create a new press in kvpress !\"\n",
    "question = \"\\nWhat is the purpose of this guide?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of How to Use Head-Specific Compression\n",
    "Hereâ€™s an example using Ada-SnapKV(AdaKV) to illustrate the process:\n",
    "\n",
    "1. Replace the Standard Flash Attention with Variable-Length Flash Attention **Before loading the LLM**.\n",
    "2. Instantiate a head-specific compression (e.g. Ada-SnapKV ) and Integrate it into the Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3285b1193cae492db18fa3ce9b9d9dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from kvpress.ada_attn import replace_var_flash_attn\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "# ckpt = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "ckpt = \"/raid/share_files/models/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# [Important] replace the vanilla flash attention with the flash_attn_varlen_func \n",
    "attn_implementation = \"flash_attention_2\"\n",
    "replace_var_flash_attn(model=ckpt)\n",
    "\n",
    "# window_size 2 for eaiser demonstration\n",
    "ada_snapkv_press = AdaSnapKVPress()\n",
    "ada_snapkv_press.compression_ratio = 0.5\n",
    "ada_snapkv_press.window_size=2\n",
    "\n",
    "# construct the pipeline\n",
    "pipe = pipeline(\"kv-press-text-generation\", model=ckpt, device=device,torch_dtype=\"auto\", model_kwargs={\"attn_implementation\":attn_implementation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The purpose of this guide is to teach users how to create a new press in kvpress, which is likely a software or tool used for creating and managing press releases or other types of content.\n"
     ]
    }
   ],
   "source": [
    "# generate text\n",
    "tokens = pipe.tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "print(\"Answer:\",pipe(context, question=question,press=ada_snapkv_press)[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Closer Look at the Role of Flattened Cache Layout in Head-Specific Compression\n",
    "\n",
    "regular cache layout: [bsz, head_num, seqlen, head_dim] -> flattened cache layout: [ (seqlen in head1, bsz1) + (seqlen in head2, bsz1) + (seqlen in head3, bsz1) ... + (seqlen in head_num, bsz), head_dim]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 heads with the same lengths of cached tokens: tensor([21, 21, 21, 21, 21, 21, 21, 21], device='cuda:0', dtype=torch.int32)\n",
      "Flatten Cache shape without press: torch.Size([168, 128])\n",
      "\n",
      "After head-specific compression, there are 8 heads with different lengths of cached tokens: tensor([ 9, 12, 11, 14,  5, 15, 13,  5], device='cuda:0', dtype=torch.int32)\n",
      "Flatten Cache shape with press: torch.Size([84, 128])\n",
      "\n",
      "Overall compression ratio: 1 - sum([21, 21, 21, 21, 21, 21, 21, 21])/sum([9, 12, 11, 14, 5, 15, 13, 5]) = 0.5\n",
      "\n",
      "Answer: The purpose of this guide is to teach users how to create a new press in kvpress, which is likely a software or tool used for creating and managing press releases or other types of content.\n"
     ]
    }
   ],
   "source": [
    "from kvpress.ada_cache import DynamicCacheSplitHeadFlatten\n",
    "\n",
    "# DynamicCacheSplitHeadFlatten Class is used to flatten the cache for all heads\n",
    "flatten_cache = DynamicCacheSplitHeadFlatten()\n",
    "with torch.no_grad():\n",
    "    outputs_without_press = pipe.model(**tokens, past_key_values=flatten_cache)\n",
    "\n",
    "flatten_cache = DynamicCacheSplitHeadFlatten()\n",
    "with torch.no_grad(), ada_snapkv_press(pipe.model):\n",
    "    output_with_press = pipe.model(**tokens, past_key_values=flatten_cache)\n",
    "\n",
    "print(f\"There are {len(outputs_without_press.past_key_values.metadata_list[0].head_lens)} heads with the same lengths of cached tokens: {outputs_without_press.past_key_values.metadata_list[0].head_lens}\")\n",
    "print(f\"Flatten Cache shape without press: {outputs_without_press.past_key_values.key_cache[0].shape}\\n\")\n",
    "\n",
    "print(f\"After head-specific compression, there are {len(output_with_press.past_key_values.metadata_list[0].head_lens)} heads with different lengths of cached tokens: {output_with_press.past_key_values.metadata_list[0].head_lens}\")\n",
    "print(f\"Flatten Cache shape with press: {output_with_press.past_key_values.key_cache[0].shape}\\n\")\n",
    "\n",
    "compress_ratio = 1 - output_with_press.past_key_values.key_cache[0].shape[0] / outputs_without_press.past_key_values.key_cache[0].shape[0]\n",
    "\n",
    "print(f\"Overall compression ratio: 1 - sum({outputs_without_press.past_key_values.metadata_list[0].head_lens.cpu().tolist()})/sum({output_with_press.past_key_values.metadata_list[0].head_lens.cpu().tolist()}) = {compress_ratio}\\n\")\n",
    "\n",
    "# The `KVPressTextGenerationPipeline` simply applies the `press` as above on the context tokens (see `_forward` method for more details).\n",
    "print(\"Answer:\",pipe(context, question=question, press=ada_snapkv_press)[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your own head-spefic cache compression method\n",
    "1. [What You Need to Do]: Implement a new cache compression method by inheriting from the `AdaBasePress` class. For the new head-specific compression method, it is recommended to generate a masked score to directly mask the KV cache pairs you wish to retain in each head, setting their score to the maximum value.\n",
    "3. The `AdaBasePress` class `forward_hook` will, by default, retain the highest-scoring cache pairs across all heads within a layer, updating the flattened cache and corresponding metadata.\n",
    "\n",
    "Below, MyAdaPress demonstrates a simple example that retains the KV cache in heads 1 and 2 in full, while the other heads retain cache according to the StreamingLLM method (i.e., keeping the 4 sink token caches and the caches within the recent window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 heads with the same lengths of cached tokens: tensor([21, 21, 21, 21, 21, 21, 21, 21], device='cuda:0', dtype=torch.int32)\n",
      "Flatten Cache shape without press: torch.Size([168, 128])\n",
      "\n",
      "After head-specific compression, there are 8 heads with different lengths of cached tokens: tensor([21, 21,  7,  7,  7,  7,  7,  7], device='cuda:0', dtype=torch.int32)\n",
      "Flatten Cache shape with press: torch.Size([84, 128])\n",
      "\n",
      "Overall compression ratio: 1 - sum([21, 21, 21, 21, 21, 21, 21, 21])/sum([21, 21, 7, 7, 7, 7, 7, 7]) = 0.5\n",
      "\n",
      "Answer: This guide is intended to help users create a new press in kvpress, which is a software for creating and managing press releases. The purpose of this guide is to provide step-by-step instructions on how to create a new press in kvpress, allowing\n"
     ]
    }
   ],
   "source": [
    "from kvpress.presses.base_press import AdaBasePress\n",
    "\n",
    "\n",
    "class MyAdaPress(AdaBasePress):\n",
    "    def score(\n",
    "        self,\n",
    "        module: nn.Module,\n",
    "        hidden_states: torch.Tensor,\n",
    "        keys: torch.Tensor,\n",
    "        values: torch.Tensor,\n",
    "        attentions: torch.Tensor,\n",
    "        kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        cache_metadata = kwargs.get(\"metadata\", None)\n",
    "        assert cache_metadata is not None, \"cache_metadata is required for AdaPress\"\n",
    "\n",
    "        # Convert to (bsz, num_key_value_heads, q_len, head_dim) for easier scoring\n",
    "        # Since this is the first compression, we can easily flatten the key and value head dimensions for score computation\n",
    "        # In multi-turn compression scenarios, extra care is needed as the key and value head dimensions may not be restructured\n",
    "        keys = keys.view(cache_metadata.bsz, cache_metadata.num_key_value_heads, -1, keys.shape[-1])\n",
    "        values = values.view(cache_metadata.bsz, cache_metadata.num_key_value_heads, -1, keys.shape[-1])\n",
    "        seq_len = keys.shape[-2]\n",
    "\n",
    "        # initialize scores\n",
    "        scores = torch.arange(seq_len, device=keys.device).float()\n",
    "        scores = scores.unsqueeze(0).unsqueeze(0).repeat(cache_metadata.bsz, cache_metadata.num_key_value_heads, 1)\n",
    "\n",
    "        max_value = torch.finfo(scores.dtype).max\n",
    "\n",
    "        # mask for attn sink\n",
    "        scores[:,:,:4] = max_value\n",
    "\n",
    "        # mask the scores for all cache in head1 and head2\n",
    "        scores[:, :2, :] = max_value\n",
    "        \n",
    "        return scores\n",
    "\n",
    "\n",
    "press = MyAdaPress(0.5)\n",
    "\n",
    "flatten_cache = DynamicCacheSplitHeadFlatten()\n",
    "with torch.no_grad():\n",
    "    outputs_without_press = pipe.model(**tokens, past_key_values=flatten_cache)\n",
    "\n",
    "flatten_cache = DynamicCacheSplitHeadFlatten()\n",
    "with torch.no_grad(), press(pipe.model):\n",
    "    output_with_press = pipe.model(**tokens, past_key_values=flatten_cache)\n",
    "\n",
    "print(f\"There are {len(outputs_without_press.past_key_values.metadata_list[0].head_lens)} heads with the same lengths of cached tokens: {outputs_without_press.past_key_values.metadata_list[0].head_lens}\")\n",
    "print(f\"Flatten Cache shape without press: {outputs_without_press.past_key_values.key_cache[0].shape}\\n\")\n",
    "\n",
    "print(f\"After head-specific compression, there are {len(output_with_press.past_key_values.metadata_list[0].head_lens)} heads with different lengths of cached tokens: {output_with_press.past_key_values.metadata_list[0].head_lens}\")\n",
    "print(f\"Flatten Cache shape with press: {output_with_press.past_key_values.key_cache[0].shape}\\n\")\n",
    "\n",
    "compress_ratio = 1 - output_with_press.past_key_values.key_cache[0].shape[0] / outputs_without_press.past_key_values.key_cache[0].shape[0]\n",
    "print(f\"Overall compression ratio: 1 - sum({outputs_without_press.past_key_values.metadata_list[0].head_lens.cpu().tolist()})/sum({output_with_press.past_key_values.metadata_list[0].head_lens.cpu().tolist()}) = {compress_ratio}\\n\")\n",
    "\n",
    "print(\"Answer:\", pipe(context, question=question, press=press)[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snapekv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
